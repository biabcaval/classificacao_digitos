{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import random\n",
    "from random import sample \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression_:\n",
    "    def __init__(self, eta=0.1, tmax=1000, bs=10):\n",
    "      self.eta = eta # learningrate -> velocidade da descida do gradiente procurando o minimo global\n",
    "      self.tmax = tmax\n",
    "      self.batch_size = bs\n",
    "\n",
    "    def fit(self, _X, _y):\n",
    "        e = 2.71828\n",
    "        X = np.array(_X)\n",
    "        y = np.array(_y)\n",
    "\n",
    "        N = X.shape[0] # número de amostras \n",
    "        d = X.shape[1]\n",
    "        self.w = np.zeros(d) # inicializando os pesos\n",
    "        \n",
    "        for t in range(self.tmax):\n",
    "            gradiente = 0\n",
    "\n",
    "            for n in range(N):\n",
    "                # soma+=(y[n]*X[n])/ (1 + e **(y[n] * (self.w).T * (t)* X[n]))\n",
    "                product = np.dot(self.w.T, X[n]) * t\n",
    "                exponent = np.exp(y[n] * product)\n",
    "                gradiente += (y[n] * X[n]) / (1 + exponent)\n",
    "\n",
    "            gradiente = gradiente * (-1/N)\n",
    "            self.w -= self.eta * gradiente\n",
    "\n",
    "            if LA.norm(gradiente) < 0.0001:\n",
    "                break\n",
    "        return self.w\n",
    "\n",
    "            \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    #funcao hipotese inferida pela regressa logistica  \n",
    "    def predict_prob(self, X):\n",
    "\n",
    "        X = np.array(X)\n",
    "        return self.sigmoid(np.dot(X, self.w)) # hipótese inferida pela regressão logistica\n",
    "\n",
    "    #predicao por classificação linear\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_prob(X)\n",
    "        return [1 if p>=0.5 else -1 for p in probs]\n",
    "    \n",
    "    def getW(self):\n",
    "        return self.w\n",
    "    \n",
    "    def getRegressionY(self, regressionX, shift=0):\n",
    "        return (-self.w[0]+shift - self.w[1]*regressionX) / self.w[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\biabc\\AppData\\Local\\Temp\\ipykernel_41192\\1308977507.py:22: RuntimeWarning: overflow encountered in exp\n",
      "  exponent = np.exp(y[n] * product)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6655405405405406\n",
      "0.6535162950257289\n"
     ]
    }
   ],
   "source": [
    "# preparando os dados para treino e teste\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "treino = pd.read_csv(\"C:/Users/biabc/OneDrive/Documents/Universidade/aprendizagem de maquina/miniprojeto_repo/classificacao_digitos/miniprojeto/datasets/treino_reduzido_15.csv\", sep=\";\")\n",
    "teste = pd.read_csv(\"C:/Users/biabc/OneDrive/Documents/Universidade/aprendizagem de maquina/miniprojeto_repo/classificacao_digitos/miniprojeto/datasets/teste_reduzido_15.csv\", sep=\";\")\n",
    "\n",
    "treino['label_treino'] = [+1 if str(i) == '1' else -1 for i in treino['label']]\n",
    "x_features = [[1, i, s] for i, s in zip(treino['intensidade'],treino['simetria'])]\n",
    "\n",
    "reglog = LogisticRegression_(eta=0.1, tmax=1000)\n",
    "reglog.fit(x_features, list(treino['label_treino'])) # treinando o modelo\n",
    "\n",
    "teste['label_teste'] = [+1 if str(i) == '1' else -1 for i in teste['label']]\n",
    "x_features_teste = [[1, i, s] for i, s in zip(teste['intensidade'],teste['simetria'])]\n",
    "# scaler = MinMaxScaler()\n",
    "# X_treino_15 = scaler.fit_transform(x_features)\n",
    "pred = reglog.predict(x_features)\n",
    "\n",
    "pred = [1 if i == 1 else 5 for i in pred]\n",
    "print(accuracy_score(treino['label'], pred))\n",
    "\n",
    "\n",
    "pred_teste = reglog.predict(x_features_teste)\n",
    "pred_teste = [1 if i == 1 else 5 for i in pred_teste]\n",
    "print(accuracy_score(teste['label'], pred_teste))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
